{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicción de Default en Préstamos — Proyecto\n",
        "\n",
        "Este notebook replica la estructura del original y completa un ETL + EDA básicos sobre el sample de Lending Club. Se incluyen justificaciones, tabla de tipos, manejo de NaNs, guardado/carga (Pickle y JSON) y un pipeline reproducible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descripción y objetivo\n",
        "- Datos: sample de Lending Club (2007–2017Q3), ~100k filas, ~150 columnas.\n",
        "- Objetivo: realizar un ETL para dejar datos en formato ‘tidy’ y preparar un EDA mínimo.\n",
        "- Entregables en este notebook: lectura robusta, tabla `(column_name, type)`, conversión de tipos, manejo de NaNs/imputación, guardado/carga de `datos_dict` (Pickle), y JSON de estrategias de imputación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "pd.options.display.max_columns = 120\n",
        "pd.options.display.width = 140\n",
        "\n",
        "DATA_URL = 'https://github.com/sonder-art/fdd_prim_2023/blob/main/codigo/pandas/LoansData_sample.csv.gz?raw=true'\n",
        "DICT_URL = 'https://resources.lendingclub.com/LCDataDictionary.xlsx'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ETL — Lectura de datos\n",
        "La lectura directa del CSV comprimido puede arrojar errores típicos (compresión, líneas problemáticas). Se resuelve con argumentos adicionales de `read_csv` (compression, engine, on_bad_lines, low_memory).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_loans(url: str = DATA_URL) -> pd.DataFrame:\n",
        "    # Manejo de compresión explícito y líneas problemáticas.\n",
        "    # Tip: dependiendo de la versión de pandas, 'compression=\"infer\"' también funciona.\n",
        "    df = pd.read_csv(\n",
        "        url,\n",
        "        compression='gzip',\n",
        "        low_memory=False,\n",
        "        engine='python',\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "    return df\n",
        "\n",
        "loans_raw = read_loans(DATA_URL)\n",
        "loans_raw.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tabla (column_name, type) — estado inicial\n",
        "Justificación breve: esta tabla ayuda a decidir conversions (porcentajes → float, fechas → datetime, categorías → category, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_column_types(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    ct = df.dtypes.reset_index().rename(columns={'index': 'column_name', 0: 'type'})\n",
        "    ct['type'] = ct['type'].astype(str)\n",
        "    return ct\n",
        "\n",
        "column_types_initial = build_column_types(loans_raw)\n",
        "column_types_initial.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargar descripción de columnas (Data Dictionary)\n",
        "Se utiliza el diccionario público del dataset y se guarda/carga en Pickle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datos_dict = pd.read_excel(DICT_URL)\n",
        "datos_dict.columns = ['feature', 'description']\n",
        "datos_dict.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pickle — guardar y cargar\n",
        "Se persiste el diccionario con Pickle para reutilizarlo localmente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar\n",
        "pickle_path = Path('datos_dict.pkl')\n",
        "datos_dict.to_pickle(pickle_path)\n",
        "# Cargar\n",
        "datos_dict2 = pd.read_pickle(pickle_path)\n",
        "assert datos_dict2.equals(datos_dict)\n",
        "datos_dict2.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversión de tipos (ETL)\n",
        "Decisiones:\n",
        "- `int_rate`, `revol_util`: strings con `%` → float en [0,1].\n",
        "- `term`: '36 months' → 36 (int).\n",
        "- `issue_d`, `earliest_cr_line`: `%b-%Y` → datetime.\n",
        "- `emp_length`: mapeo a años (0–10).\n",
        "- Categóricas: `home_ownership`, `purpose`, `grade`, `sub_grade`, `verification_status`, `loan_status`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def coerce_types(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    # Fechas\n",
        "    for col, fmt in [('issue_d', '%b-%Y'), ('earliest_cr_line', '%b-%Y')]:\n",
        "        if col in out.columns:\n",
        "            out[col] = pd.to_datetime(out[col], format=fmt, errors='coerce')\n",
        "    # Porcentajes a fracción\n",
        "    for col in ['int_rate', 'revol_util']:\n",
        "        if col in out.columns:\n",
        "            out[col] = pd.to_numeric(out[col].astype(str).str.rstrip('%'), errors='coerce') / 100.0\n",
        "    # Term en meses\n",
        "    if 'term' in out.columns:\n",
        "        out['term'] = pd.to_numeric(out['term'].astype(str).str.extract(r'(\\d+)')[0], errors='coerce')\n",
        "    # Años de empleo\n",
        "    if 'emp_length' in out.columns:\n",
        "        mapping = {\n",
        "            '10+ years': 10, '9 years': 9, '8 years': 8, '7 years': 7, '6 years': 6,\n",
        "            '5 years': 5, '4 years': 4, '3 years': 3, '2 years': 2, '1 year': 1,\n",
        "            '< 1 year': 0, 'n/a': np.nan, 'NaN': np.nan\n",
        "        }\n",
        "        out['emp_length'] = pd.to_numeric(out['emp_length'].replace(mapping), errors='coerce')\n",
        "    # Categóricas\n",
        "    for catcol in ['home_ownership','purpose','grade','sub_grade','verification_status','loan_status']:\n",
        "        if catcol in out.columns:\n",
        "            out[catcol] = out[catcol].astype('category')\n",
        "    return out\n",
        "\n",
        "loans_typed = coerce_types(loans_raw)\n",
        "build_column_types(loans_typed).head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manejo de NaNs o missings\n",
        "Estrategia simple y justificable:\n",
        "- Numéricas: imputar con mediana (robusta a outliers).\n",
        "- Fechas: imputar con la moda (valor más frecuente).\n",
        "- Categóricas/objetos: usar identificador `'missing'` (permite rastrear imputaciones).\n",
        "Se registra todo en un JSON `{col: {estrategia, valor}}` para reproducibilidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_imputation_strategies(df: pd.DataFrame) -> dict:\n",
        "    strategies = {}\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        if pd.api.types.is_numeric_dtype(s):\n",
        "            val = float(s.median()) if s.notna().any() else 0.0\n",
        "            strategies[col] = {'estrategia': 'median', 'valor': val}\n",
        "        elif pd.api.types.is_datetime64_any_dtype(s):\n",
        "            m = s.dropna()\n",
        "            val = m.mode().iloc[0].isoformat() if not m.empty else None\n",
        "            strategies[col] = {'estrategia': 'mode', 'valor': val}\n",
        "        elif pd.api.types.is_categorical_dtype(s) or pd.api.types.is_object_dtype(s):\n",
        "            m = s.dropna().mode()\n",
        "            val = (m.iloc[0] if len(m) > 0 else 'missing')\n",
        "            strategies[col] = {'estrategia': 'identificador', 'valor': str(val if pd.notna(val) else 'missing')}\n",
        "        else:\n",
        "            strategies[col] = {'estrategia': 'identificador', 'valor': 'missing'}\n",
        "    return strategies\n",
        "\n",
        "def save_strategies_json(strategies: dict, path: str | Path = 'imputacion_strategies.json') -> Path:\n",
        "    path = Path(path)\n",
        "    with path.open('w', encoding='utf-8') as f:\n",
        "        json.dump(strategies, f, ensure_ascii=False, indent=2)\n",
        "    return path\n",
        "\n",
        "def load_strategies_json(path: str | Path = 'imputacion_strategies.json') -> dict:\n",
        "    with Path(path).open('r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def apply_imputation(df: pd.DataFrame, strategies: dict) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for col, spec in strategies.items():\n",
        "        if col not in out.columns:\n",
        "            continue\n",
        "        val = spec.get('valor', None)\n",
        "        if pd.api.types.is_datetime64_any_dtype(out[col]) and isinstance(val, str):\n",
        "            fill = pd.to_datetime(val, errors='coerce')\n",
        "            out[col] = out[col].fillna(fill)\n",
        "        else:\n",
        "            out[col] = out[col].fillna(val)\n",
        "    return out\n",
        "\n",
        "strategies = build_imputation_strategies(loans_typed)\n",
        "save_path = save_strategies_json(strategies)\n",
        "strategies_loaded = load_strategies_json(save_path)\n",
        "loans_imputed = apply_imputation(loans_typed, strategies_loaded)\n",
        "loans_imputed.isna().sum().head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA — limpieza adicional y reproducibilidad\n",
        "- Quitar columnas inservibles (IDs, URLs, descripciones libres) si existen.\n",
        "- Mantener funciones para repetir el proceso de forma determinista.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DROP_COLUMNS = [\n",
        "    'id', 'member_id', 'url', 'desc', 'title'\n",
        "]\n",
        "\n",
        "def drop_unhelpful(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    cols = [c for c in DROP_COLUMNS if c in df.columns]\n",
        "    return df.drop(columns=cols)\n",
        "\n",
        "loans_clean = drop_unhelpful(loans_imputed)\n",
        "loans_clean.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline reproducible (función única)\n",
        "Encapsula lectura → tipos → imputación → drop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def etl_pipeline(\n",
        "    data_url: str = DATA_URL,\n",
        "    dict_url: str = DICT_URL\n",
        "):\n",
        "    loans = read_loans(data_url)\n",
        "    loans = coerce_types(loans)\n",
        "    col_types = build_column_types(loans)\n",
        "    # Diccionario de datos (además de Pickle para cache local)\n",
        "    dd = pd.read_excel(dict_url)\n",
        "    dd.columns = ['feature', 'description']\n",
        "    dd.to_pickle('datos_dict.pkl')\n",
        "    # Imputación\n",
        "    strategies = build_imputation_strategies(loans)\n",
        "    save_strategies_json(strategies, 'imputacion_strategies.json')\n",
        "    loans = apply_imputation(loans, strategies)\n",
        "    # Limpieza extra\n",
        "    loans = drop_unhelpful(loans)\n",
        "    return loans, col_types, strategies\n",
        "\n",
        "# Ejecución (opcional)\n",
        "# loans_final, types_final, strategies_final = etl_pipeline()\n",
        "# loans_final.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notas y justificaciones\n",
        "- Tipos: se prioriza convertir a tipos nativos (`datetime64`, `float`, `int`, `category`) para habilitar análisis eficiente.\n",
        "- Imputación: mediana en numéricos evita sesgo por outliers; identificador `'missing'` permite trazar valores imputados en categóricas.\n",
        "- Reproducibilidad: funciones puras y archivo JSON documentan decisiones de limpieza.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
